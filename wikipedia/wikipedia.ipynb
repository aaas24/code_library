{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wiki_dump_reader\n",
      "  Downloading wiki-dump-reader-0.0.4.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wiki_dump_reader\n",
      "  Building wheel for wiki_dump_reader (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wiki_dump_reader: filename=wiki_dump_reader-0.0.4-py3-none-any.whl size=4000 sha256=5fd0c0f1e7ab187de9e945945db0ecc0709a06413a5e07d66f6136b1c90ba859\n",
      "  Stored in directory: /Users/alialvarez/Library/Caches/pip/wheels/72/7b/e1/2b758cc4aa080655ef16ed4f759232491118494fe54ce003d2\n",
      "Successfully built wiki_dump_reader\n",
      "Installing collected packages: wiki_dump_reader\n",
      "Successfully installed wiki_dump_reader-0.0.4\n"
     ]
    }
   ],
   "source": [
    "#add missing packages to kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install wiki_dump_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Downloading the entire Wikipedia Repository "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targeted files\n",
    "url= 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2'\n",
    "filename=os.path.join(os.getcwd(),'enwiki-latest-pages-articles.xml.bz2')\n",
    "location=os.getcwd()\n",
    "\n",
    "logfile=os.path.join(os.getcwd(),\"log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate log file\n",
    "f = open(logfile, \"a\")\n",
    "f.truncate(0)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading latest wiki dump, decompressing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout = subprocess.PIPE, \n",
    "        stderr = subprocess.PIPE,\n",
    "        text = True, \n",
    "        shell = True\n",
    "    )\n",
    "    std_out, std_err = process.communicate()\n",
    "    if verbose:\n",
    "        print(std_out.strip(), std_err)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Convert to code when wanting to download. Slow download\n",
    "# deletes previous file\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# download latest wiki dump \n",
    "runcmd(\"wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "#create backup of bz2 file\n",
    "try:\n",
    "    os.remove(filename+'.backup')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "runcmd(\"cp enwiki-latest-pages-articles.xml enwiki-latest-pages-articles.xml.bz2.backup\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and cleaning enwiki-latest-pages-articles.xml.bz2 to enwiki-latest-pages-articles.txt... Traceback (most recent call last):\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 645, in <module>\n",
      "    main()\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 640, in main\n",
      "    process_dump(input_file, args.templates, output_path, file_size,\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 336, in process_dump\n",
      "    templates = load_templates(input, template_file)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 209, in load_templates\n",
      "    for line in file:\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/bz2.py\", line 195, in read1\n",
      "    return self._buffer.read1(size)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/_compression.py\", line 99, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decompress file using #wikiextractor library\n",
    "runcmd(\"bzip2 -d /Users/alialvarez/Desktop/STUDIES/github/code_library/wikipedia/enwiki-latest-pages-articles.xml.bz2\", verbose = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "#create backup of xml file\n",
    "try:\n",
    "    os.remove(filename[:-4]+'.backup')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "runcmd(\"cp enwiki-latest-pages-articles.xml enwiki-latest-pages-articles.xml.backup\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Wiki dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['419020', '|Lăutari|', '0', '1083044321', '2022-04-16T17:29:47Z', '', '', '22291']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['434551', '|TQ|', '0', '1054704638', '2021-11-11T17:04:12Z', '', '', '734']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['560934', '|Glenmont station|', '0', '1083409549', '2022-04-18T17:23:03Z', '', '', '18431']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['1035712', '|Counts per minute|', '0', '1044124399', '2021-09-13T17:59:22Z', '', '', '9060']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['1404360', '|Rachel, Nevada|', '0', '1080720063', '2022-04-03T01:01:19Z', '', '', '20233']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['2111177', '|Roddy McCorley|', '0', '1084082088', '2022-04-22T13:06:59Z', '', '', '8948']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['2179981', '|Kisa|', '0', '1028950442', '2021-06-17T00:17:00Z', '', '', '1315']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['2563072', '|Manfred Max-Neef|', '0', '1029083264', '2021-06-17T19:54:32Z', '', '', '7540']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['3620303', '|Explosive detection|', '0', '1085225364', '2022-04-29T05:20:54Z', '', '', '17385']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['4533642', '|Knightswood Secondary School|', '0', '1087935571', '2022-05-15T09:25:13Z', '', '', '9454']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['4542806', '|Parthenolide|', '0', '1085884668', '2022-05-02T23:49:00Z', '', '', '8123']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['4559576', '|West Iron County School District|', '0', '46060788', '2006-03-29T20:46:01Z', '', '', '38']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['7671898', '|Érik Boisse|', '0', '1072124886', '2022-02-16T03:07:35Z', '', '', '3359']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['9313935', '|Wheaton High School|', '0', '1083331061', '2022-04-18T08:09:09Z', '', '', '13691']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['14313333', '|Climate of Kolkata|', '0', '1087943868', '2022-05-15T10:43:42Z', '', '', '4847']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['15920386', '|Columbia University School of Professional Studies|', '0', '1078929895', '2022-03-24T02:29:42Z', '', '', '18050']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['16081936', '|American-Mexican War|', '0', '964557027', '2020-06-26T06:41:28Z', '', '', '36']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['18558983', '|Category:Wikipedians from the Upper Peninsula of Michigan|', '14', '350330051', '2010-03-17T02:53:02Z', '', '', '800']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['18765770', '|Ongudaysky District|', '0', '1070117864', '2022-02-05T19:46:42Z', '', '', '4815']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['19157709', '|Jean Baptiste Baudreau II|', '0', '1088717813', '2022-05-19T19:00:42Z', '', '', '5278']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['20182708', '|Kathy Byron|', '0', '1088205724', '2022-05-16T18:51:31Z', '', '', '16252']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['21620170', '|Romanian peasant music|', '0', '1083045505', '2022-04-16T17:38:48Z', '', '', '1548']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['23814911', '|RIAA v. Joel Tenenbaum|', '0', '463033209', '2011-11-29T04:22:49Z', '', '', '55']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['23816241', '|Joel Tenenbaum|', '0', '463033263', '2011-11-29T04:23:09Z', '', '', '55']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['23875064', '|Reba Som|', '0', '1074073506', '2022-02-26T07:51:55Z', '', '', '3053']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['24068657', '|RIAA vs. Joel Tenenbaum|', '0', '463033321', '2011-11-29T04:23:26Z', '', '', '55']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['29299532', '|Brandun DeShay|', '0', '1088275954', '2022-05-17T03:21:39Z', '', '', '7628']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['30634026', '|Joint Base Myer–Henderson Hall|', '0', '1083395911', '2022-04-18T16:05:59Z', '', '', '7609']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['31003283', '|Dutch Burghers|', '0', '1080846425', '2022-04-03T20:21:48Z', '', '', '7582']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['32956540', '|Highly emetogenic chemotherapy|', '0', '448226866', '2011-09-03T14:02:18Z', '', '', '54']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['34992479', '|Wikipedia:Motto of the day/March 8, 2012|', '4', '480626041', '2012-03-07T08:01:23Z', '', '', '123']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['34992497', '|Wikipedia:Motto of the day/March 7, 2012|', '4', '480626202', '2012-03-07T08:02:53Z', '', '', '33']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['35002660', '|Wikipedia:Motto of the day/March 15, 2012|', '4', '480798715', '2012-03-08T07:16:40Z', '', '', '138']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['35006261', '|Wikipedia:Motto of the day/March 16, 2012|', '4', '480856417', '2012-03-08T16:39:25Z', '', '', '103']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['35006269', '|Wikipedia:Motto of the day/March 17, 2012|', '4', '480856500', '2012-03-08T16:40:07Z', '', '', '83']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['35006274', '|Wikipedia:Motto of the day/March 18, 2012|', '4', '480856592', '2012-03-08T16:40:42Z', '', '', '42']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['36024342', '|Huang Wenshiung|', '0', '495739703', '2012-06-03T09:05:03Z', '', '', '25']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['36026430', '|Peter Wen-shiung Huang|', '0', '495783189', '2012-06-03T15:23:42Z', '', '', '25']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['36026676', '|Wikipedia:WikiProject India/Assessment/Tag & Assess 2012/Special Awards|', '4', '495788339', '2012-06-03T15:59:35Z', '', '', '904']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['36205850', '|Wikipedia:Motto of the day/July 26, 2012|', '4', '498650288', '2012-06-21T11:40:31Z', '', '', '62']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['36571346', '|Orange County High School of the Arts|', '0', '504675106', '2012-07-29T00:41:04Z', '', '', '46']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['37247911', '|Talitres|', '0', '1074743338', '2022-03-01T23:09:57Z', '', '', '8195']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['38599275', '|Shadow and Bone|', '0', '1088829869', '2022-05-20T10:14:13Z', '', '', '12712']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['39545563', '|Wikipedia:Motto of the day/Invite a member|', '4', '557796486', '2013-06-01T07:10:37Z', '', '', '34']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['39641975', '|Wikipedia:GARC|', '4', '559343973', '2013-06-11T05:37:11Z', '', '', '88']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['44006850', '|Onoffice|', '0', '1039654385', '2021-08-19T23:45:32Z', '', '', '1960']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['46732450', '|WDID-LD|', '0', '1077629783', '2022-03-17T10:10:52Z', '', '', '2725']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['48219433', '|2001 Billboard Music Awards|', '0', '1044502235', '2021-09-15T15:32:37Z', '', '', '5917']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['49014202', '|EA Sports UFC 2|', '0', '1085416659', '2022-04-30T13:00:29Z', '', '', '22402']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['50982838', '|Ursitory|', '0', '1082943602', '2022-04-16T02:03:50Z', '', '', '4317']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['51415212', '|Tram route 9 (Antwerp)|', '0', '1077147109', '2022-03-14T19:08:10Z', '', '', '5945']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['51525443', '|Narelle Oliver|', '0', '1047663703', '2021-10-01T23:28:38Z', '', '', '5739']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['54185525', '|Wikipedia:Long-term abuse/Country music category vandal from Tennessee|', '4', '1083350581', '2022-04-18T10:51:18Z', '', '', '11227']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['61562084', '|Eastmark High School|', '0', '1076424508', '2022-03-11T01:55:07Z', '', '', '4332']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['64258200', '|Miracle in the Desert: The Rise and Fall of the Salton Sea|', '0', '996176697', '2020-12-24T23:44:08Z', '', '', '4711']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['65825642', \"|Wikipedia:April Fools/April Fools' Day 2021|\", '4', '1083996709', '2022-04-22T00:44:01Z', '', '', '54650']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['65926438', '|National Anthem of the Republic of the Rif|', '0', '1059799777', '2021-12-11T18:13:27Z', '', '', '8013']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['67498560', '|M-84AS1|', '0', '1080568438', '2022-04-02T01:08:09Z', '', '', '21148']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['67692973', '|Organ language|', '0', '1063290919', '2022-01-02T05:48:26Z', '', '', '5382']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['69858063', '|Wikipedia:Long-term abuse/Bucharest Wild Kratts and horror film vandal|', '4', '1082625536', '2022-04-14T05:53:12Z', '', '', '8354']\n",
      "The following line has incomplete info and therefore it's been removed from the dataset:\n",
      "['70552409', '|Draft:Meena caste category|', '118', '1083110833', '2022-04-17T03:00:24Z', '', '', '4168']\n",
      "Done processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(filename[:-8]+'.csv')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "import wiki_dump_parser as parser\n",
    "parser.xml_to_csv('enwiki-latest-pages-articles.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alialvarez/opt/anaconda3/envs/wikipedia/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (9,10,11,12,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_ns</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>contributor_name</th>\n",
       "      <th>bytes</th>\n",
       "      <th>COLUMN_1</th>\n",
       "      <th>COLUMN_2</th>\n",
       "      <th>COLUMN_3</th>\n",
       "      <th>COLUMN_4</th>\n",
       "      <th>COLUMN_5</th>\n",
       "      <th>COLUMN_6</th>\n",
       "      <th>COLUMN_7</th>\n",
       "      <th>COLUMN_8</th>\n",
       "      <th>COLUMN_9</th>\n",
       "      <th>COLUMN_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>|AccessibleComputing|</td>\n",
       "      <td>0</td>\n",
       "      <td>1002250816</td>\n",
       "      <td>2021-01-23T15:15:01Z</td>\n",
       "      <td>20842734</td>\n",
       "      <td>|Elli|</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>|Anarchism|</td>\n",
       "      <td>0</td>\n",
       "      <td>1085838220</td>\n",
       "      <td>2022-05-02T18:53:41Z</td>\n",
       "      <td>31382403</td>\n",
       "      <td>|BappleBusiness|</td>\n",
       "      <td>105122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>|AfghanistanHistory|</td>\n",
       "      <td>0</td>\n",
       "      <td>783865149</td>\n",
       "      <td>2017-06-05T04:18:18Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>|AfghanistanGeography|</td>\n",
       "      <td>0</td>\n",
       "      <td>783865160</td>\n",
       "      <td>2017-06-05T04:18:23Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>|AfghanistanPeople|</td>\n",
       "      <td>0</td>\n",
       "      <td>783865293</td>\n",
       "      <td>2017-06-05T04:19:42Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>|AfghanistanCommunications|</td>\n",
       "      <td>0</td>\n",
       "      <td>783865299</td>\n",
       "      <td>2017-06-05T04:19:45Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>|AfghanistanTransportations|</td>\n",
       "      <td>0</td>\n",
       "      <td>783821589</td>\n",
       "      <td>2017-06-04T21:42:11Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>|AfghanistanMilitary|</td>\n",
       "      <td>0</td>\n",
       "      <td>1071065800</td>\n",
       "      <td>2022-02-10T17:56:28Z</td>\n",
       "      <td>8066546</td>\n",
       "      <td>|Xqbot|</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>|AfghanistanTransnationalIssues|</td>\n",
       "      <td>0</td>\n",
       "      <td>783821743</td>\n",
       "      <td>2017-06-04T21:43:14Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>|AssistiveTechnology|</td>\n",
       "      <td>0</td>\n",
       "      <td>783865310</td>\n",
       "      <td>2017-06-05T04:19:50Z</td>\n",
       "      <td>9784415</td>\n",
       "      <td>|Tom.Reding|</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id                        page_title page_ns revision_id  \\\n",
       "0       10             |AccessibleComputing|       0  1002250816   \n",
       "1       12                       |Anarchism|       0  1085838220   \n",
       "2       13              |AfghanistanHistory|       0   783865149   \n",
       "3       14            |AfghanistanGeography|       0   783865160   \n",
       "4       15               |AfghanistanPeople|       0   783865293   \n",
       "5       18       |AfghanistanCommunications|       0   783865299   \n",
       "6       19      |AfghanistanTransportations|       0   783821589   \n",
       "7       20             |AfghanistanMilitary|       0  1071065800   \n",
       "8       21  |AfghanistanTransnationalIssues|       0   783821743   \n",
       "9       23             |AssistiveTechnology|       0   783865310   \n",
       "\n",
       "              timestamp contributor_id  contributor_name   bytes COLUMN_1  \\\n",
       "0  2021-01-23T15:15:01Z       20842734            |Elli|     111      NaN   \n",
       "1  2022-05-02T18:53:41Z       31382403  |BappleBusiness|  105122      NaN   \n",
       "2  2017-06-05T04:18:18Z        9784415      |Tom.Reding|      90      NaN   \n",
       "3  2017-06-05T04:18:23Z        9784415      |Tom.Reding|      92      NaN   \n",
       "4  2017-06-05T04:19:42Z        9784415      |Tom.Reding|      95      NaN   \n",
       "5  2017-06-05T04:19:45Z        9784415      |Tom.Reding|      97      NaN   \n",
       "6  2017-06-04T21:42:11Z        9784415      |Tom.Reding|     113      NaN   \n",
       "7  2022-02-10T17:56:28Z        8066546           |Xqbot|      92      NaN   \n",
       "8  2017-06-04T21:43:14Z        9784415      |Tom.Reding|     101      NaN   \n",
       "9  2017-06-05T04:19:50Z        9784415      |Tom.Reding|      88      NaN   \n",
       "\n",
       "  COLUMN_2 COLUMN_3 COLUMN_4 COLUMN_5 COLUMN_6 COLUMN_7 COLUMN_8 COLUMN_9  \\\n",
       "0      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "5      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "6      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "7      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "8      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "9      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "   COLUMN_10  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "7        NaN  \n",
       "8        NaN  \n",
       "9        NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=pd.read_csv('enwiki-latest-pages-articles_metadata_example.csv')\n",
    "df=pd.DataFrame(output)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above example, the wiki_dump_parser only allows to retrieve basic metadata information (like page_id), but does not render the markup information. Additionally, the parser does not handle well some of the lines, forcing the values to extend to 10 other columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki dump reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_reader import Cleaner, iterate\n",
    "\n",
    "wiki={}\n",
    "cleaner = Cleaner()\n",
    "\n",
    "for title, text in iterate('enwiki-latest-pages-articles.xml'):\n",
    "    orig_text=text\n",
    "    text = cleaner.clean_text(text)\n",
    "    cleaned_text, links = cleaner.build_links(text)\n",
    "\n",
    "    #add files to dictionary\n",
    "    wiki.update({title:[cleaned_text]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AccessibleComputing</th>\n",
       "      <td>REDIRECT Computer accessibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anarchism</th>\n",
       "      <td>Anarchism is a political philosophy and moveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AfghanistanHistory</th>\n",
       "      <td>REDIRECT History of Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AfghanistanGeography</th>\n",
       "      <td>REDIRECT Geography of Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AfghanistanPeople</th>\n",
       "      <td>REDIRECT Demographics of Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                cleaned\n",
       "AccessibleComputing                     REDIRECT Computer accessibility\n",
       "Anarchism             Anarchism is a political philosophy and moveme...\n",
       "AfghanistanHistory                      REDIRECT History of Afghanistan\n",
       "AfghanistanGeography                  REDIRECT Geography of Afghanistan\n",
       "AfghanistanPeople                  REDIRECT Demographics of Afghanistan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki=pd.DataFrame.from_dict(wiki, orient='index')\n",
    "df_wiki.columns=['cleaned']\n",
    "df_wiki.to_csv(os.path.join(os.getcwd(), 'wiki_dump_example.csv'))\n",
    "df_wiki.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Other WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runcmd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cb77350915b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mruncmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./extract_and_clean_wiki_dump.sh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'runcmd' is not defined"
     ]
    }
   ],
   "source": [
    "runcmd(\"./extract_and_clean_wiki_dump.sh\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikiextractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Downloading a Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our subject targeted is: \n",
    "subject='ted talk speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain a search on the subject:\n",
    "import wikipedia\n",
    "\n",
    "search_result = wikipedia.search(subject)\n",
    "print('the search result is:')\n",
    "print(search_result)\n",
    "result=search_result[1]\n",
    "print('')\n",
    "print('the selected page is:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain a URL base on the search result\n",
    "url=(result).replace(\" \", \"_\")\n",
    "url= 'https://en.wikipedia.org/wiki/'+ url\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain website\n",
    "result=wikipedia.page(result)\n",
    "\n",
    "#parse attributes library provides\n",
    "title=result.title\n",
    "summary = result.summary\n",
    "categories=result.categories\n",
    "content = result.content\n",
    "links = result.links\n",
    "references = result.references\n",
    "\n",
    "# print info\n",
    "print(\"Page content:\\n\", content, \"\\n\")\n",
    "print(\"Page title:\", title, \"\\n\")\n",
    "print(\"Categories:\", categories, \"\\n\")\n",
    "print(\"Links:\", links, \"\\n\")\n",
    "print(\"References:\", references, \"\\n\")\n",
    "print(\"Summary:\", summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wikipedia Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Page title:\", title, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=result.html()\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=pd.read_html(url)\n",
    "\n",
    "print(type(html))\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using beautiful soup to dig into the html\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "html = urlopen (url)\n",
    "bsObj = BeautifulSoup(html.read (), 'html.parser')\n",
    "print(bsObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we obtain the html code of the target website, we can use different methods to extract the parts of the text we wish to obtain. \n",
    "\n",
    "### The first method is using the findAll function in BeautifulSoup Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tag):\n",
    "    \"\"\"\n",
    "    This function extracts the list of tags and returns the list. It uses the findAll function of BeautifulSoup\n",
    "    \"\"\"\n",
    "    soup=bsObj\n",
    "    a=[]\n",
    "    content=soup.findAll(tag)\n",
    "    for item in content:\n",
    "        a.append(item.get_text())\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors= (tag_list(\"td\")\n",
    "          [6:] #remove first 6 values that do not correspond to authors or talks\n",
    "         )\n",
    "authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second method is using Regular Expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "#Parse HTML to find urls of speakers \n",
    "txt = str(bsObj)\n",
    "result=[]\n",
    "# reg='(?<=a href=)(.*)(?=title)'\n",
    "# reg='(?<=data-sort-value=)(.*)(?=title)'\n",
    "reg='(?<=td>)(.*)(?=</td>)'\n",
    "\n",
    "reobj = re.compile(reg)\n",
    "for matchobj in reobj.finditer(txt):\n",
    "    result.append(matchobj[1 ])\n",
    "\n",
    "for line in result:\n",
    "    print (line) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography & Refrences\n",
    "\n",
    "Sources: \n",
    "    [Wiki-Dump-Reader Package](https://github.com/CyberZHG/wiki-dump-reader)\n",
    "    [How-to Article](https://www.heatonresearch.com/2017/03/03/python-basic-wikipedia-parsing.html)\n",
    "    [Use Markup](http://pediapress.com/code/)\n",
    "    [Library extractor](https://pythonawesome.com/a-python-tool-for-extracting-plain-text-from-wikipedia-dumps/)\n",
    "    [Example of cleaning dump](https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia",
   "language": "python",
   "name": "wikipedia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
