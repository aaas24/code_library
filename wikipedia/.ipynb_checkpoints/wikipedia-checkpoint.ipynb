{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wiki-dump-parser in /Users/alialvarez/opt/anaconda3/envs/wikipedia/lib/python3.8/site-packages (2.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "#add missing packages to kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install wiki-dump-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file location\n",
    "cwd=\"/Users/alialvarez/Desktop/STUDIES/github/code_library/wikipedia/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targeted files\n",
    "url= 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2'\n",
    "filename=os.path.join(cwd,'enwiki-latest-pages-articles.xml.bz2')\n",
    "location=cwd\n",
    "\n",
    "logfile=os.path.join(cwd,\"log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate log file\n",
    "f = open(logfile, \"a\")\n",
    "f.truncate(0)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Downloading the entire Wikipedia Repository "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading latest wiki dump, decompressing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout = subprocess.PIPE, \n",
    "        stderr = subprocess.PIPE,\n",
    "        text = True, \n",
    "        shell = True\n",
    "    )\n",
    "    std_out, std_err = process.communicate()\n",
    "    if verbose:\n",
    "        print(std_out.strip(), std_err)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes previous file\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#download latest wiki dump\n",
    "runcmd(\"wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2; echo 'download finished' \", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and cleaning enwiki-latest-pages-articles.xml.bz2 to enwiki-latest-pages-articles.txt... Traceback (most recent call last):\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 645, in <module>\n",
      "    main()\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 640, in main\n",
      "    process_dump(input_file, args.templates, output_path, file_size,\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 336, in process_dump\n",
      "    templates = load_templates(input, template_file)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/site-packages/wikiextractor/WikiExtractor.py\", line 209, in load_templates\n",
      "    for line in file:\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/bz2.py\", line 195, in read1\n",
      "    return self._buffer.read1(size)\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/Users/alialvarez/opt/anaconda3/lib/python3.8/_compression.py\", line 99, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decompress file using #wikiextractor library\n",
    "runcmd(\"./extract_and_clean_wiki_dump.sh\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Wiki dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d6e20669117f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwiki_dump_parser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enwiki-latest-pages-articles.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/wikipedia/lib/python3.8/site-packages/wiki_dump_parser.py\u001b[0m in \u001b[0;36mxml_to_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;31m# Initializing xml parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartElementHandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml'"
     ]
    }
   ],
   "source": [
    "import wiki_dump_parser as parser\n",
    "parser.xml_to_csv('enwiki-latest-pages-articles.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Downloading a Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our subject targeted is: \n",
    "subject='ted talk speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain a search on the subject:\n",
    "import wikipedia\n",
    "\n",
    "search_result = wikipedia.search(subject)\n",
    "print('the search result is:')\n",
    "print(search_result)\n",
    "result=search_result[1]\n",
    "print('')\n",
    "print('the selected page is:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain a URL base on the search result\n",
    "url=(result).replace(\" \", \"_\")\n",
    "url= 'https://en.wikipedia.org/wiki/'+ url\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=pd.read_html(url)\n",
    "\n",
    "print(type(html))\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wikipedia Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain website\n",
    "result=wikipedia.page(result)\n",
    "\n",
    "#parse attributes library provides\n",
    "title=result.title\n",
    "summary = result.summary\n",
    "categories=result.categories\n",
    "content = result.content\n",
    "links = result.links\n",
    "references = result.references\n",
    "\n",
    "# print info\n",
    "print(\"Page content:\\n\", content, \"\\n\")\n",
    "print(\"Page title:\", title, \"\\n\")\n",
    "print(\"Categories:\", categories, \"\\n\")\n",
    "print(\"Links:\", links, \"\\n\")\n",
    "print(\"References:\", references, \"\\n\")\n",
    "print(\"Summary:\", summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Page title:\", title, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=result.html()\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using beautiful soup to dig into the html\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "html = urlopen (url)\n",
    "bsObj = BeautifulSoup(html.read (), 'html.parser')\n",
    "print(bsObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we obtain the html code of the target website, we can use different methods to extract the parts of the text we wish to obtain. \n",
    "\n",
    "### The first method is using the findAll function in BeautifulSoup Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tag):\n",
    "    \"\"\"\n",
    "    This function extracts the list of tags and returns the list. It uses the findAll function of BeautifulSoup\n",
    "    \"\"\"\n",
    "    soup=bsObj\n",
    "    a=[]\n",
    "    content=soup.findAll(tag)\n",
    "    for item in content:\n",
    "        a.append(item.get_text())\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors= (tag_list(\"td\")\n",
    "          [6:] #remove first 6 values that do not correspond to authors or talks\n",
    "         )\n",
    "authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second method is using Regular Expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "#Parse HTML to find urls of speakers \n",
    "txt = str(bsObj)\n",
    "result=[]\n",
    "# reg='(?<=a href=)(.*)(?=title)'\n",
    "# reg='(?<=data-sort-value=)(.*)(?=title)'\n",
    "reg='(?<=td>)(.*)(?=</td>)'\n",
    "\n",
    "reobj = re.compile(reg)\n",
    "for matchobj in reobj.finditer(txt):\n",
    "    result.append(matchobj[1 ])\n",
    "\n",
    "for line in result:\n",
    "    print (line) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography & Refrences\n",
    "\n",
    "Sources: \n",
    "    [Wiki-Dump-Reader Package](https://github.com/CyberZHG/wiki-dump-reader)\n",
    "    [How-to Article](https://www.heatonresearch.com/2017/03/03/python-basic-wikipedia-parsing.html)\n",
    "    [Use Markup](http://pediapress.com/code/)\n",
    "    [Library extractor](https://pythonawesome.com/a-python-tool-for-extracting-plain-text-from-wikipedia-dumps/)\n",
    "    [Example of cleaning dump](https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia",
   "language": "python",
   "name": "wikipedia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
